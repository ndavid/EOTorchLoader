{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ae43f-a45d-40d9-8798-fe08c71de610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import os\n",
    "from os import listdir\n",
    "import random\n",
    "import itertools\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b7ac7-9bc8-46fc-b420-c5165e1556de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main parameters\n",
    "# change to our own directory with INRIA Aerial Image Dataset\n",
    "inria_dataset_root_dir = Path(\"/media/dlsupport/DATA1/EOData/INRIA/AerialImageDataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f73bb0-0793-4c94-9a97-22c45a11050c",
   "metadata": {},
   "source": [
    "# Define INRIA Dataset dataframe\n",
    "\n",
    "The INRIA Aerial Image Labeling Dataset has the following structure : \n",
    "\n",
    "<!-- language: lang-none -->\n",
    "\n",
    "    .\n",
    "    ├── test\n",
    "    │   └── images\n",
    "    └── train\n",
    "        ├── gt\n",
    "        └── images\n",
    "        \n",
    "Each image has a filename of type {town_prefix}{i}.tif with i in [1:36] Gt (ground truth) and image has the same filename.\n",
    "First we load info of train images with *load_geo_img_dir* utils function of EOTorchLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1cae28-98cd-4bb1-9725-c5bf0e66a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from eotorchloader.dataset.utils import load_geo_img_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba72c98-03a1-4406-955f-0a2c7c9b25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_train_val_df = load_geo_img_dir(inria_dataset_root_dir/\"train\"/\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f51fe-1075-4e6b-a64e-b76dbbfe0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_train_val_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560b180-b9e4-4b2a-975f-a0e2b1aa9956",
   "metadata": {},
   "source": [
    "then we define a split in train data between train and val dataset. This could be done in multiple way.\n",
    "\n",
    " * set val image as first images of each town. Usually for INRIA dataset val images are set as the 6 first image of each towns.\n",
    " * set val as all the image in a town and train as the images in all other towns.\n",
    " \n",
    "To represent theses two splits we add 2 columns to the extracted dataframe :\n",
    " \n",
    " * a column \"town\" with the town id/name\n",
    " * a column \"standart_split\" with take value in [\"train\", \"val\", \"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad75ca-4c44-4072-82c3-d1114f32dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First add a town column.\n",
    "# This is done by splitting the name string values (as \"tyrol-w28\") in 2 parts : a name with alphabetic character (\"tyrol-w\") and a id/num with numerical values (28)\n",
    "inria_train_val_df[['town', 'num']] = inria_train_val_df[\"name\"].str.extract('([a-zA-Z\\-]+)([^a-zA-Z\\-]+)', expand=True)\n",
    "# convert num from string to int\n",
    "inria_train_val_df['num'] = inria_train_val_df['num'].astype(int)\n",
    "\n",
    "# Next we add standard_split columns\n",
    "inria_train_val_df[\"standard_split\"] = \"train\" # first initialize all row with train\n",
    "inria_train_val_df.loc[inria_train_val_df[\"num\"]<=6 ,\"standard_split\"] = \"val\" # set all row/image with num < 6 as validation data\n",
    "\n",
    "# finally we rename path as img_path and add a gt_path columns with corresponding mask path\n",
    "inria_train_val_df = inria_train_val_df.rename(columns={\"path\": \"img_path\"})\n",
    "inria_train_val_df[\"msk_path\"] =  inria_train_val_df[\"img_path\"].str.replace(\"images\", \"gt\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356585b-7c57-4c0e-ad74-68b214f7ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inria_train_val_df[['name','town', 'num', 'standard_split']]) \n",
    "print(inria_train_val_df['town'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73796e26-363a-4c67-9130-1bcc3e12e825",
   "metadata": {},
   "source": [
    "Once we have the list of image and mask we could intialize a TorchDataset which crop the image.\n",
    "\n",
    " * the tile_size is set in pixel\n",
    " * by default no transofmr is apply and the sample are in form {\"image\" : np.array, \"mask\" :np.array } in channel first order (CHW or rasterio like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74716df9-4559-4ccd-929b-df4474660cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for use in train code\n",
    "from eotorchloader.dataset.scene_dataset import LargeImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd02a0d-405c-4d4d-8854-37f0746481ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_train_df = inria_train_val_df[inria_train_val_df[\"standard_split\"]==\"train\"]\n",
    "image_files_train = inria_train_df[\"img_path\"].values\n",
    "mask_files_train = inria_train_df[\"msk_path\"].values\n",
    "print(image_files_train[0:5])\n",
    "print(mask_files_train[0:5])\n",
    "\n",
    "train_dataset_tile = LargeImageDataset(\n",
    "    image_files=image_files_train,\n",
    "    mask_files=mask_files_train,\n",
    "    tile_size = 512,\n",
    "    transforms=None,\n",
    "    image_bands=[1,2,3],\n",
    "    mask_bands=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a931c-e0c7-49dc-a534-7494eabfb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 201\n",
    "test_data = train_dataset_tile[test_idx]\n",
    "print(f\" keys : {test_data.keys()}\")\n",
    "img_shape =  test_data['image'].shape\n",
    "msk_shape = test_data['mask'].shape\n",
    "print(f\" image shape : {img_shape}, mask shape : {msk_shape}\")\n",
    "print(f\" mask type : {test_data['mask'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2804cb-a021-443c-85fa-6c274abb5e11",
   "metadata": {},
   "source": [
    "To display a sample or batch we could use some display function of EoTorchloader. \n",
    "As sample are not transformed from original format, to display mask a lut/nomenclature could be useful.\n",
    "\n",
    "First we need to check what value are present in a mask array. In INRIA case it should be \n",
    "\n",
    " * 0 for no bati\n",
    " * 255 for bati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3918df7-18d5-45fc-98a4-3105d8c7ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.histogram(test_data['mask'], bins=10))\n",
    "print(np.unique(test_data['mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0393e3b-9573-4448-9838-fc199eee8e1b",
   "metadata": {},
   "source": [
    "### configure tranform for training\n",
    "\n",
    "for pytorch training we need to scale input image and mask to [0-1] range and to convert to float tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c6f24-b8de-4a9b-858e-56aa41d0c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eotorchloader.transform.tensor import ToTorchTensor\n",
    "from eotorchloader.transform.scale import ScaleImageToFloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c320c-2402-4237-9349-923b021cf0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms for training\n",
    "inria_train_tf = [\n",
    "    ScaleImageToFloat(scale_factor=255, clip=True, img_only=False),\n",
    "    ToTorchTensor()\n",
    "]\n",
    "\n",
    "train_dataset_tile_b = LargeImageDataset(\n",
    "    image_files=image_files_train,\n",
    "    mask_files=mask_files_train,\n",
    "    tile_size = 384,\n",
    "    transforms=inria_train_tf,\n",
    "    image_bands=[1,2,3],\n",
    "    mask_bands=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94841cdc-ee40-4155-856f-5dcb7dc3a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 240\n",
    "test_data = train_dataset_tile_b[test_idx]\n",
    "print(f\" keys : {test_data.keys()}\")\n",
    "img_shape =  test_data['image'].shape\n",
    "msk_shape = test_data['mask'].shape\n",
    "print(f\" image shape : {img_shape}, mask shape : {msk_shape}\")\n",
    "print(f\" mask type : {test_data['mask'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018d76f-3464-4e70-b9cc-a764e1166a15",
   "metadata": {},
   "source": [
    "### use transform to display sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff713c82-cc22-4d92-9ba4-2fa96b83b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "inria_lut = np.array([\n",
    " [  0, 255,  255, 255], # white \n",
    " [  255, 255, 50, 150]  # pink\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd794c1-1d0f-4d4a-be6b-d288f5a8c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display import\n",
    "from eotorchloader.transform.tensor import CHW_to_HWC\n",
    "from eotorchloader.transform.display import ToRgbDisplay\n",
    "\n",
    "from eotorchloader.display.matplotlib import view_patch, view_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4bc230-d532-4313-b944-d307dcea99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_last_transform = CHW_to_HWC(img_only=True)\n",
    "display_patch_transform = ToRgbDisplay(lut=inria_lut, flatten_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315edb60-8a5e-4d1d-9d47-5ee024c01e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = train_dataset_tile[142]\n",
    "view_patch(test_data, transforms=[channel_last_transform, display_patch_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7df89-85bf-4ed7-8f33-74391c52be66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EOtorchLoader)",
   "language": "python",
   "name": "eotorchloader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
